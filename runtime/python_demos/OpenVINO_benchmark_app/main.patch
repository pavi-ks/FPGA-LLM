--- /nfs/site/disks/swip_dla_1/resources/inference_engine/2024.6.0_with_dev_tools/1/linux64/suse15/python/openvino/tools/benchmark/main.py	2025-01-31 08:51:46.481486000 -0500
+++ main.py	2025-01-15 14:46:47.184837000 -0500
@@ -1,4 +1,4 @@
-# Copyright (C) 2018-2024 Altera Corporation
+# Copyright (C) 2018-2023 Altera Corporation
 # SPDX-License-Identifier: Apache-2.0
 
 import os
@@ -7,11 +7,11 @@
 
 from openvino.runtime import Dimension,properties
 
-from openvino.tools.benchmark.benchmark import Benchmark
+import benchmark as openvino_benchmark
 from openvino.tools.benchmark.parameters import parse_args
 from openvino.tools.benchmark.utils.constants import MULTI_DEVICE_NAME, \
     CPU_DEVICE_NAME, GPU_DEVICE_NAME, \
-    BLOB_EXTENSION, AUTO_DEVICE_NAME
+    BIN_EXTENSION, AUTO_DEVICE_NAME
 from openvino.tools.benchmark.utils.inputs_filling import get_input_data
 from openvino.tools.benchmark.utils.logging import logger
 from openvino.tools.benchmark.utils.utils import next_step, get_number_iterations, pre_post_processing, \
@@ -41,13 +41,13 @@
     if args.report_type == "average_counters" and MULTI_DEVICE_NAME in args.target_device:
         raise Exception("only detailed_counters report type is supported for MULTI device")
 
-    _, ext = os.path.splitext(args.path_to_model)
-    is_network_compiled = True if ext == BLOB_EXTENSION else False
-    is_precisiton_set = not (args.input_precision == "" and args.output_precision == "" and args.input_output_precision == "")
+    if args.number_infer_requests != 1 and "FPGA" in args.target_device:
+        logger.warning(f"If the target FPGA design uses JTAG to access the CSRs on the FPGA AI Suite IP "\
+                       "(e.g. the Agilex 5E Premium Development Kit JTAG Design Example), "\
+                       "then the number of inference request must be 1.")
 
-    if is_network_compiled and is_precisiton_set:
-        raise Exception("Cannot set precision for a compiled model. " \
-                        "Please re-compile your model with required precision.")
+    _, ext = os.path.splitext(args.path_to_model)
+    is_network_compiled = True if ext == BIN_EXTENSION else False
 
     return args, is_network_compiled
 
@@ -84,7 +84,7 @@
         # ------------------------------ 2. Loading OpenVINO Runtime -------------------------------------------
         next_step(step_id=2)
 
-        benchmark = Benchmark(args.target_device, args.number_infer_requests,
+        benchmark = openvino_benchmark.Benchmark(args.target_device, args.number_infer_requests,
                               args.number_iterations, args.time, args.api_type, args.inference_only)
 
         if args.extensions:
@@ -106,7 +106,7 @@
         next_step()
 
         def set_performance_hint(device):
-            perf_hint = properties.hint.PerformanceMode.THROUGHPUT
+            perf_hint = properties.hint.PerformanceMode.UNDEFINED
             supported_properties = benchmark.core.get_property(device, properties.supported_properties())
             if properties.hint.performance_mode() in supported_properties:
                 if is_flag_set_in_command_line('hint'):
@@ -117,16 +117,16 @@
                     elif args.perf_hint == "cumulative_throughput" or args.perf_hint == "ctput":
                         perf_hint = properties.hint.PerformanceMode.CUMULATIVE_THROUGHPUT
                     elif args.perf_hint=='none':
-                        # Not set PerformanceMode, and plugin will apply its internal default PerformanceMode
-                        return
+                        perf_hint = properties.hint.PerformanceMode.UNDEFINED
                     else:
                         raise RuntimeError("Incorrect performance hint. Please set -hint option to"
                             "`throughput`(tput), `latency', 'cumulative_throughput'(ctput) value or 'none'.")
                 else:
-                    perf_hint = properties.hint.PerformanceMode.LATENCY if benchmark.api_type == "sync" else properties.hint.PerformanceMode.THROUGHPUT
+                    perf_hint = properties.hint.PerformanceMode.THROUGHPUT if benchmark.api_type == "async" else properties.hint.PerformanceMode.LATENCY
                     logger.warning(f"Performance hint was not explicitly specified in command line. " +
                     f"Device({device}) performance hint will be set to {perf_hint}.")
-                config[device][properties.hint.performance_mode()] = perf_hint
+                if perf_hint != properties.hint.PerformanceMode.UNDEFINED:
+                    config[device][properties.hint.performance_mode()] = perf_hint
             else:
                 logger.warning(f"Device {device} does not support performance hint property(-hint).")
 
@@ -166,8 +166,11 @@
             supported_properties = benchmark.core.get_property(device, properties.supported_properties())
             if device not in config.keys():
                 config[device] = {}
-
             ## high-level performance modes
+            # The orginial OV 2022.3 Python API fails with the pc flag, so we comment it out
+            # for both the HETERO and FPGA devices in our patched version of the Python demos
+            if device in ['HETERO', 'FPGA']:
+                continue
             set_performance_hint(device)
 
             if is_flag_set_in_command_line('nireq'):
@@ -315,13 +318,9 @@
                     del device_number_streams[device]
 
         device_config = {}
-        # In case of multiple devices found prefer the one given in CLI argument
-        if benchmark.device.find(device_name) == 0 and device_name in config.keys():
-            device_config = config[device_name]
-        else:
-            for device in config:
-                if benchmark.device.find(device) == 0:
-                    device_config = config[device]
+        for device in config:
+            if benchmark.device.find(device) == 0:
+                device_config = config[device]
         if args.cache_dir:
             benchmark.set_cache_dir(args.cache_dir)
 
@@ -433,16 +432,21 @@
             next_step()
 
             start_time = datetime.utcnow()
-            compiled_model = benchmark.core.import_model(args.path_to_model, benchmark.device, device_config)
-            duration_ms = f"{(datetime.utcnow() - start_time).total_seconds() * 1000:.2f}"
-            logger.info(f"Import model took {duration_ms} ms")
-            if statistics:
-                statistics.add_parameters(StatisticsReport.Category.EXECUTION_RESULTS,
-                                          [
-                                              ('import model time (ms)', duration_ms)
-                                          ])
-            app_inputs_info, _ = get_inputs_info(args.shape, args.data_shape, args.layout, args.batch_size, args.scale_values, args.mean_values, compiled_model.inputs)
-            batch_size = get_network_batch_size(app_inputs_info)
+            try:
+                with open(args.path_to_model, "rb") as model_stream:
+                    model_bytes = model_stream.read()
+                compiled_model = benchmark.core.import_model(model_bytes, device_name)
+                duration_ms = f"{(datetime.utcnow() - start_time).total_seconds() * 1000:.2f}"
+                logger.info(f"Import model took {duration_ms} ms")
+                if statistics:
+                    statistics.add_parameters(StatisticsReport.Category.EXECUTION_RESULTS,
+                                            [
+                                                ('import model time (ms)', duration_ms)
+                                            ])
+                app_inputs_info, _ = get_inputs_info(args.shape, args.data_shape, args.layout, args.batch_size, args.scale_values, args.mean_values, compiled_model.inputs)
+                batch_size = get_network_batch_size(app_inputs_info)
+            except Exception as e:
+                raise RuntimeError(f"Cannot open or import compiled model file: {args.path_to_model}. Error: {str(e)}")
 
         # --------------------- 8. Querying optimal runtime parameters --------------------------------------------------
         next_step()
@@ -451,7 +455,7 @@
         keys = compiled_model.get_property(properties.supported_properties())
         logger.info("Model:")
         for k in keys:
-            skip_keys = (properties.supported_properties())
+            skip_keys = ('SUPPORTED_METRICS', 'SUPPORTED_CONFIG_KEYS', properties.supported_properties())
             if k not in skip_keys:
                 value = compiled_model.get_property(k)
                 if k == properties.device.properties():
@@ -473,7 +477,28 @@
                 device_number_streams[device] = compiled_model.get_property(key)
 
         # ------------------------------------ 9. Creating infer requests and preparing input data ----------------------
-        next_step()
+        next_step()      
+        try:
+            # Check number of inference requests for FPGA
+            num_instance = benchmark.core.get_property(device_name="FPGA", property="COREDLA_NUM_INSTANCES")
+            max_num_infer_requests_per_instance = benchmark.core.get_property(device_name="FPGA", property="COREDLA_MAX_NUMBER_INFERENCE_REQUESTS_PER_INSTANCE")
+            # round up the quotient to the nearest integer
+        except Exception as e:
+            if "Failed to fetch FPGA property" in str(e):
+                # there is no graph mapped to the FPGA in this case
+                num_instance = 1
+                max_num_infer_requests_per_instance = benchmark.nireq
+                if "FPGA" in device_name:
+                    logger.warning("Target device specifies an FPGA, but no subgraph from any input model can be mapped to the FPGA.")
+            else:
+                logger.exception("Unanticipated exception occured while trying to query properties from the FPGA plugin: {e}")
+            
+        # Make sure the number of inference request is no higher than the hardware/runtime bound
+        # If the bound is non-positive, then the device is emulator and there is no need to perform the check
+        current_num_infer_requests_per_instance = (benchmark.nireq + num_instance - 1) // num_instance
+        if ((current_num_infer_requests_per_instance > max_num_infer_requests_per_instance) and (max_num_infer_requests_per_instance > 0)):
+            raise Exception(f"To use this FPGA design in async mode, make sure the number of inference requests per instance is no greater than {max_num_infer_requests_per_instance}")
+
 
         # Create infer requests
         requests = benchmark.create_infer_requests(compiled_model)
@@ -657,7 +682,7 @@
             exeDevice = compiled_model.get_property("EXECUTION_DEVICES")
             logger.info(f'Execution Devices:{exeDevice}')
         except:
-            pass
+            exeDevice = None
         logger.info(f'Count:            {iteration} iterations')
         logger.info(f'Duration:         {get_duration_in_milliseconds(total_duration_sec):.2f} ms')
         if MULTI_DEVICE_NAME not in device_name:
@@ -696,4 +721,4 @@
                 [('error', str(e))]
             )
             statistics.dump()
-        sys.exit(1)
+        sys.exit(1)
\ No newline at end of file
